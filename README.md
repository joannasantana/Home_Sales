# Home_Sales

The purpose of this challenge was to use SparkSQL to analyze "big data" to determine key metrics about home sales data. 

Skills demonstrated:
- Using pyspark to read in an AWS S3 bucket and transform a csv into a dataframe.
- Creating a temporary view to run SparkSQL queries.
- Writing and executing SparkSQL queries.
- Using Spark to partition the data, cache and uncache a temporary table, and verify that the table has been uncached.
- Comparing query runtimes between different methods (cache, partition, parquet).
- Using Google Collab.